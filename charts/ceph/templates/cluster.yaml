---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: {{ .Values.storage.name }}-cluster
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: quay.io/ceph/ceph:v18
    allowUnsupported: true
  mon:
    count: {{ ceil (divf .Values.storage.nodes 3) }}
    allowMultiplePerNode: true
  mgr:
    count: {{ ceil (divf .Values.storage.nodes 3) }}
    allowMultiplePerNode: true
  dashboard:
    enabled: true
    ssl: false
  crashCollector:
    disable: true
  storage:
    useAllNodes: true
    useAllDevices: true
  monitoring:
    enabled: false
  healthCheck:
    daemonHealth:
      mon:
        interval: 45s
        timeout: 600s
  priorityClassNames:
    all: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
  cephConfig:
    global:
      osd_pool_default_size: "1"
      mon_warn_on_pool_no_redundancy: "false"
      bdev_flock_retry: "20"
      bluefs_buffered_io: "false"
      mon_data_avail_warn: "10"
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: {{ .Values.storage.name }}-mgr
  namespace: rook-ceph
spec:
  name: {{ .Values.storage.blockname }}
  failureDomain: {{ .Values.storage.failureDomain }}
  replicated:
    size: {{ .Values.storage.replicas }}
    # requireSafeReplicaSize: false
---
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: {{ .Values.storage.name }}-filesystem
  namespace: rook-ceph # namespace:cluster
spec:
  metadataPool:
    replicated:
      size: {{ .Values.storage.metadataReplicas }} # TODO
      requireSafeReplicaSize: false
  dataPools:
    - name: replicated
      failureDomain: {{ .Values.storage.failureDomain }}
      replicated:
        size: {{ .Values.storage.replicas }}
        # requireSafeReplicaSize: false
  preserveFilesystemOnDelete: true
  metadataServer:
    activeCount: 1 # TODO
    # activeStandby: false
---
# create default csi subvolume group
apiVersion: ceph.rook.io/v1
kind: CephFilesystemSubVolumeGroup
metadata:
  name: {{ .Values.storage.name }}-filesystem-csi
  namespace: rook-ceph
spec:
  # The name of the subvolume group. If not set, the default is the name of the subvolumeGroup CR.
  name: csi
  # filesystemName is the metadata name of the CephFilesystem CR where the subvolume group will be created
  filesystemName: {{ .Values.storage.name }}-filesystem
  # reference https://docs.ceph.com/en/latest/cephfs/fs-volumes/#pinning-subvolumes-and-subvolume-groups
  # only one out of (export, distributed, random) can be set at a time
  # by default pinning is set with value: distributed=1
  # for disabling default values set (distributed=0)
  pinning:
    distributed: 1            # distributed=<0, 1> (disabled=0)
    # export:                 # export=<0-256> (disabled=-1)
    # random:                 # random=[0.0, 1.0](disabled=0.0)
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the secret namespaces are defined
  clusterID: {{ .Release.Namespace }}

  # CephFS filesystem name into which the volume shall be created
  fsName: {{ .Values.storage.name }}-filesystem

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: {{ .Values.storage.name }}-filesystem-replicated

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ .Release.Namespace }}
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ .Release.Namespace }}
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ .Release.Namespace }}
allowVolumeExpansion: true
reclaimPolicy: Delete